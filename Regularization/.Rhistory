# Fit the Random Forest model
rf_model <- randomForest(output ~ ., data = train_data, ntree = 100)
# Make predictions on the test set
predictions_rf <- predict(rf_model, newdata = test_x)
# Evaluate the performance of the model
conf_matrix_rf <- table(predictions_rf, test_y)
print(conf_matrix_rf)
# Feature Selection (Choose the features you want to include)
selected_features <- c("Age", "RestingBP", "Cholesterol", "MaxHR", "Oldpeak")
# Subset the data with selected features
train_data <- train_data[, selected_features]
install.packages("reshape2")
library(readr)
heart <- read_csv("heart.csv")
View(heart)
#missing value check
colSums(is.na(heart))
#data is clean there arent any missing values
#are our categorical data is also classified into numeric one, its important for us to separate them.
#lets divide the data into categorical,continous and target variable
cat_cols <- c('sex', 'exng', 'caa', 'cp', 'fbs', 'restecg', 'slp', 'thall')
con_cols <- c('age', 'trtbps', 'chol', 'thalachh', 'oldpeak')
target_col <- c('output')
# Print the column information
cat("The categorial variables are: ", cat_cols, "\n")
cat("The continuous variable are: ", con_cols, "\n")
cat("The target variable is: ", target_col, "\n")
#gettting the summary of the data
summary(heart)
#Checking for outliers for Age
library(reshape2)
# Create box plots for numerical variables to identify outliers
numericals <- c("age", "trtbps", "chol", "thalachh", "oldpeak")
# Create box plots
boxplot_data <- melt(heart[, c("output", numericals)], id.vars = "output")
ggplot(boxplot_data, aes(x = variable, y = value, color = factor(output))) +
geom_boxplot() +
facet_wrap(~variable, scales = "free_y", ncol = 1) +
labs(title = "Boxplot of Numerical Variables by Heart Disease Status") +
theme_minimal()
library(ggplot2)
library(knitr)
library(ggplot2)
library(dplyr)
library(tidyverse)
library(RColorBrewer)
library(magrittr)
library(readxl)
library(kableExtra)
library(ggrepel)
library(tidyr)
library(leaps)
library(ISLR)
library(caret)
library(corrplot)
library(ggplot2)
library(glmnet)
library(corrplot)
library(readr)
heart <- read_csv("heart.csv")
View(heart)
#missing value check
colSums(is.na(heart))
#data is clean there arent any missing values
#are our categorical data is also classified into numeric one, its important for us to separate them.
#lets divide the data into categorical,continous and target variable
cat_cols <- c('sex', 'exng', 'caa', 'cp', 'fbs', 'restecg', 'slp', 'thall')
con_cols <- c('age', 'trtbps', 'chol', 'thalachh', 'oldpeak')
target_col <- c('output')
# Print the column information
cat("The categorial variables are: ", cat_cols, "\n")
cat("The continuous variable are: ", con_cols, "\n")
cat("The target variable is: ", target_col, "\n")
#gettting the summary of the data
summary(heart)
#Checking for outliers for Age
library(reshape2)
# Create box plots for numerical variables to identify outliers
numericals <- c("age", "trtbps", "chol", "thalachh", "oldpeak")
# Create box plots
boxplot_data <- melt(heart[, c("output", numericals)], id.vars = "output")
ggplot(boxplot_data, aes(x = variable, y = value, color = factor(output))) +
geom_boxplot() +
facet_wrap(~variable, scales = "free_y", ncol = 1) +
labs(title = "Boxplot of Numerical Variables by Heart Disease Status") +
theme_minimal()
#ask sahil to upliad this and find out how to solve outliers
#Now that our data is clean we can start with data modeling.
#As we are trying to predict about heart attacks, we should be using classification models.
#Feature Engineering
# Set the seed for reproducibility
set.seed(42)
# Assuming your data frame is named 'df' and the target variable is named 'output'
# Create indices for train and test sets
indices <- createDataPartition(heart$output, p = 0.8, list = FALSE)
# Split the data into training and testing sets
train_data <- heart[indices, ]
test_data <- heart[-indices, ]
# Separate predictors (X) and target variable (y) for training set
train_x <- train_data[, !(names(train_data) %in% "output")]
train_y <- train_data$output
# Separate predictors (X) and target variable (y) for testing set
test_x <- test_data[, !(names(test_data) %in% "output")]
test_y <- test_data$output
# Print the shapes of the datasets
cat("The shape of train_x is ", dim(train_x), "\n")
cat("The shape of train_y is ", length(train_y), "\n")
cat("The shape of test_x is ", dim(test_x), "\n")
cat("The shape of test_y is ", length(test_y), "\n")
# Install and load the required library
library(rpart)
# Assuming your data frame is named 'df' and the target variable is named 'output'
# Assuming train_x, train_y, test_x, and test_y are already defined
# Combine predictors and target variable for training set
train_data <- cbind(train_x, output = train_y)
# Fit the decision tree model
dt_model <- rpart(output ~ ., data = train_data, method = "class")
# Print the decision tree
printcp(dt_model)
# Plot the decision tree (you may need to install the 'rpart.plot' package)
library(rpart.plot)
rpart.plot(dt_model, main = "Decision Tree")
# Make predictions on the test set
predictions_tree <- predict(dt_model, newdata = test_x, type = "class")
# Evaluate the performance of the model
conf_matrix_tree <- table(predictions_tree, test_y)
print(conf_matrix_tree)
# Install and load the required library
library(randomForest)
# Assuming your data frame is named 'df' and the target variable is named 'output'
# Assuming train_x, train_y, test_x, and test_y are already defined
# Combine predictors and target variable for training set
train_data <- cbind(train_x, output = train_y)
# Fit the Random Forest model
rf_model <- randomForest(output ~ ., data = train_data, ntree = 100)
# Make predictions on the test set
predictions_rf <- predict(rf_model, newdata = test_x)
# Evaluate the performance of the model
conf_matrix_rf <- table(predictions_rf, test_y)
print(conf_matrix_rf)
# Feature Selection (Choose the features you want to include)
selected_features <- c("Age", "RestingBP", "Cholesterol", "MaxHR", "Oldpeak")
# Subset the data with selected features
train_data <- train_data[, selected_features]
selected_features <- c("age", "trtbps", "Chol", "thalachh", "oldpeak")
train_data <- train_data[, selected_features]
selected_features <- c("age", "trtbps", "chol", "thalachh", "oldpeak")
# Subset the data with selected features
train_data <- train_data[, selected_features]
test_data <- test_data[, selected_features]
# Standardize/Scale the features (optional but recommended for k-means)
train_data_scaled <- scale(train_data)
test_data_scaled <- scale(test_data)
# Perform k-means clustering
num_clusters <- 2  # You can adjust the number of clusters
kmeans_result <- kmeans(train_data_scaled, centers = num_clusters)
# Access the cluster assignments
cluster_assignments <- kmeans_result$cluster
# Add cluster assignments to the training data
train_data_clustered <- cbind(train_data, Cluster = cluster_assignments)
# Display the clustered data
print(train_data_clustered)
summary(train_data_clustered)
#Checking for outliers for Age
library(reshape2)
# Create box plots for numerical variables to identify outliers
numericals <- c("age", "trtbps", "chol", "thalachh", "oldpeak")
# Create box plots
boxplot_data <- melt(heart[, c("output", numericals)], id.vars = "output")
ggplot(boxplot_data, aes(x = variable, y = value, color = factor(output))) +
geom_boxplot() +
facet_wrap(~variable, scales = "free_y", ncol = 1) +
labs(title = "Boxplot of Numerical Variables by Heart Disease Status") +
theme_minimal()
#Feature Engineering
# Set the seed for reproducibility
set.seed(42)
# Assuming your data frame is named 'df' and the target variable is named 'output'
# Create indices for train and test sets
indices <- createDataPartition(heart$output, p = 0.8, list = FALSE)
# Split the data into training and testing sets
train_data <- heart[indices, ]
test_data <- heart[-indices, ]
# Separate predictors (X) and target variable (y) for training set
train_x <- train_data[, !(names(train_data) %in% "output")]
train_y <- train_data$output
# Separate predictors (X) and target variable (y) for testing set
test_x <- test_data[, !(names(test_data) %in% "output")]
test_y <- test_data$output
# Print the shapes of the datasets
cat("The shape of train_x is ", dim(train_x), "\n")
cat("The shape of train_y is ", length(train_y), "\n")
cat("The shape of test_x is ", dim(test_x), "\n")
cat("The shape of test_y is ", length(test_y), "\n")
# Combine predictors and target variable for training set
train_data <- cbind(train_x, output = train_y)
# Fit the decision tree model
dt_model <- rpart(output ~ ., data = train_data, method = "class")
# Print the decision tree
printcp(dt_model)
# Plot the decision tree (you may need to install the 'rpart.plot' package)
library(rpart.plot)
rpart.plot(dt_model, main = "Decision Tree")
# Make predictions on the test set
predictions_tree <- predict(dt_model, newdata = test_x, type = "class")
# Evaluate the performance of the model
conf_matrix_tree <- table(predictions_tree, test_y)
print(conf_matrix_tree)
# Install and load the required library
library(randomForest)
# Assuming your data frame is named 'df' and the target variable is named 'output'
# Assuming train_x, train_y, test_x, and test_y are already defined
# Combine predictors and target variable for training set
train_data <- cbind(train_x, output = train_y)
# Fit the Random Forest model
rf_model <- randomForest(output ~ ., data = train_data, ntree = 100)
# Make predictions on the test set
predictions_rf <- predict(rf_model, newdata = test_x)
# Evaluate the performance of the model
conf_matrix_rf <- table(predictions_rf, test_y)
print(conf_matrix_rf)
# Install and load the required library
library(randomForest)
# Assuming your data frame is named 'df' and the target variable is named 'output'
# Assuming train_x, train_y, test_x, and test_y are already defined
# Combine predictors and target variable for training set
train_data <- cbind(train_x, output = train_y)
# Fit the Random Forest model
rf_model <- randomForest(output ~ ., data = train_data, ntree = 100, type = "class")
# Make predictions on the test set
predictions_rf <- predict(rf_model, newdata = test_x)
# Evaluate the performance of the model
conf_matrix_rf <- table(predictions_rf, test_y)
print(conf_matrix_rf)
# Feature Selection (Choose the features you want to include)
selected_features <- c("age", "trtbps", "chol", "thalachh", "oldpeak")
# Subset the data with selected features
train_data <- train_data[, selected_features]
test_data <- test_data[, selected_features]
# Standardize/Scale the features (optional but recommended for k-means)
train_data_scaled <- scale(train_data)
test_data_scaled <- scale(test_data)
# Perform k-means clustering
num_clusters <- 2  # You can adjust the number of clusters
kmeans_result <- kmeans(train_data_scaled, centers = num_clusters)
# Access the cluster assignments
cluster_assignments <- kmeans_result$cluster
# Add cluster assignments to the training data
train_data_clustered <- cbind(train_data, Cluster = cluster_assignments)
# Display the clustered data
print(train_data_clustered)
plot(train_data_scaled, col = kmeans_obj$cluster)
plot(train_data_scaled, col = kmeans_result$cluster)
library(readr)
heart <- read_csv("heart.csv")
View(heart)
#missing value check
colSums(is.na(heart))
#data is clean there arent any missing values
#are our categorical data is also classified into numeric one, its important for us to separate them.
#lets divide the data into categorical,continous and target variable
cat_cols <- c('sex', 'exng', 'caa', 'cp', 'fbs', 'restecg', 'slp', 'thall')
con_cols <- c('age', 'trtbps', 'chol', 'thalachh', 'oldpeak')
target_col <- c('output')
# Print the column information
cat("The categorial variables are: ", cat_cols, "\n")
cat("The continuous variable are: ", con_cols, "\n")
cat("The target variable is: ", target_col, "\n")
set.seed(42) # For reproducibility
library(caret)
splitIndex <- createDataPartition(heart$output, p = .8, list = FALSE, times = 1)
train_data <- heart[splitIndex,]
test_data <- heart[-splitIndex,]
# Standardize the features (excluding the target variable 'output')
preproc <- preProcess(train_data[,-ncol(train_data)], method = c("center", "scale"))
train_data_scaled <- predict(preproc, train_data[,-ncol(train_data)])
test_data_scaled <- predict(preproc, test_data[,-ncol(test_data)])
# Adding the output column back
train_data_scaled$output <- train_data$output
test_data_scaled$output <- test_data$output
library(glmnet)
x_train <- as.matrix(train_data_scaled[,-ncol(train_data_scaled)]) # Features matrix for training
y_train <- train_data_scaled$output # Response variable for training, ensure it's a factor for classification
train_data <- heart[splitIndex,]
train_data
train_data1 <- heart[indices, ]
train_data1
# Separate predictors (X) and target variable (y) for training set
train_x <- train_data[, !(names(train_data) %in% "output")]
train_y <- train_data$output
train_x
#Checking for outliers for Age
library(reshape2)
# Create box plots for numerical variables to identify outliers
numericals <- c("age", "trtbps", "chol", "thalachh", "oldpeak")
# Create box plots
boxplot_data <- melt(heart[, c("output", numericals)], id.vars = "output")
ggplot(boxplot_data, aes(x = variable, y = value, color = factor(output))) +
geom_boxplot() +
facet_wrap(~variable, scales = "free_y", ncol = 1) +
labs(title = "Boxplot of Numerical Variables by Heart Disease Status") +
theme_minimal()
#Now that our data is clean we can start with data modeling.
#As we are trying to predict about heart attacks, we should be using classification models.
#Feature Engineering
# Set the seed for reproducibility
set.seed(42)
# Assuming your data frame is named 'df' and the target variable is named 'output'
# Create indices for train and test sets
indices <- createDataPartition(heart$output, p = 0.8, list = FALSE)
# Split the data into training and testing sets
train_data <- heart[indices, ]
test_data <- heart[-indices, ]
# Separate predictors (X) and target variable (y) for training set
train_x <- train_data[, !(names(train_data) %in% "output")]
train_y <- train_data$output
# Separate predictors (X) and target variable (y) for testing set
test_x <- test_data[, !(names(test_data) %in% "output")]
test_y <- test_data$output
# Print the shapes of the datasets
cat("The shape of train_x is ", dim(train_x), "\n")
cat("The shape of train_y is ", length(train_y), "\n")
cat("The shape of test_x is ", dim(test_x), "\n")
cat("The shape of test_y is ", length(test_y), "\n")
# Install and load the required library
library(rpart)
# Assuming your data frame is named 'df' and the target variable is named 'output'
# Assuming train_x, train_y, test_x, and test_y are already defined
# Combine predictors and target variable for training set
train_data <- cbind(train_x, output = train_y)
# Fit the decision tree model
dt_model <- rpart(output ~ ., data = train_data, method = "class")
# Print the decision tree
printcp(dt_model)
# Plot the decision tree (you may need to install the 'rpart.plot' package)
library(rpart.plot)
rpart.plot(dt_model, main = "Decision Tree")
# Make predictions on the test set
predictions_tree <- predict(dt_model, newdata = test_x, type = "class")
# Evaluate the performance of the model
conf_matrix_tree <- table(predictions_tree, test_y)
print(conf_matrix_tree)
# Feature Selection (Choose the features you want to include)
selected_features <- c("age", "trtbps", "chol", "thalachh", "oldpeak")
# Subset the data with selected features
train_data <- train_data[, selected_features]
test_data <- test_data[, selected_features]
# Standardize/Scale the features (optional but recommended for k-means)
train_data_scaled <- scale(train_data)
test_data_scaled <- scale(test_data)
# Perform k-means clustering
num_clusters <- 2  # You can adjust the number of clusters
kmeans_result <- kmeans(train_data_scaled, centers = num_clusters)
# Access the cluster assignments
cluster_assignments <- kmeans_result$cluster
# Add cluster assignments to the training data
train_data_clustered <- cbind(train_data, Cluster = cluster_assignments)
# Display the clustered data
print(train_data_clustered)
plot(train_data_scaled, col = kmeans_result$cluster)
#Feature Engineering
# Set the seed for reproducibility
set.seed(42)
# Assuming your data frame is named 'df' and the target variable is named 'output'
# Create indices for train and test sets
indices <- createDataPartition(heart$output, p = 0.8, list = FALSE)
# Split the data into training and testing sets
train_data <- heart[indices, ]
test_data <- heart[-indices, ]
# Separate predictors (X) and target variable (y) for training set
train_x <- train_data[, !(names(train_data) %in% "output")]
train_y <- train_data$output
# Separate predictors (X) and target variable (y) for testing set
test_x <- test_data[, !(names(test_data) %in% "output")]
test_y <- test_data$output
# Print the shapes of the datasets
cat("The shape of train_x is ", dim(train_x), "\n")
cat("The shape of train_y is ", length(train_y), "\n")
cat("The shape of test_x is ", dim(test_x), "\n")
cat("The shape of test_y is ", length(test_y), "\n")
# Install and load the required library
library(rpart)
# Assuming your data frame is named 'df' and the target variable is named 'output'
# Assuming train_x, train_y, test_x, and test_y are already defined
# Combine predictors and target variable for training set
train_data <- cbind(train_x, output = train_y)
# Fit the decision tree model
dt_model <- rpart(output ~ ., data = train_data, method = "class")
# Print the decision tree
printcp(dt_model)
# Plot the decision tree (you may need to install the 'rpart.plot' package)
library(rpart.plot)
rpart.plot(dt_model, main = "Decision Tree")
# Make predictions on the test set
predictions_tree <- predict(dt_model, newdata = test_x, type = "class")
# Evaluate the performance of the model
conf_matrix_tree <- table(predictions_tree, test_y)
print(conf_matrix_tree)
# Feature Selection (Choose the features you want to include)
selected_features <- c("age", "trtbps", "chol", "thalachh", "oldpeak")
# Subset the data with selected features
train_data <- train_data[, selected_features]
test_data <- test_data[, selected_features]
# Standardize/Scale the features (optional but recommended for k-means)
train_data_scaled <- scale(train_data)
test_data_scaled <- scale(test_data)
# Perform k-means clustering
num_clusters <- 2  # You can adjust the number of clusters
kmeans_result <- kmeans(train_data_scaled, centers = num_clusters)
# Access the cluster assignments
cluster_assignments <- kmeans_result$cluster
# Add cluster assignments to the training data
train_data_clustered <- cbind(train_data, Cluster = cluster_assignments)
# Display the clustered data
print(train_data_clustered)
plot(train_data_scaled, col = kmeans_result$cluster)
library(knitr)
library(ggplot2)
library(dplyr)
library(tidyverse)
library(RColorBrewer)
library(magrittr)
library(readxl)
library(kableExtra)
library(ggrepel)
library(tidyr)
library(leaps)
library(ISLR)
library(caret)
library(corrplot)
library(ggplot2)
library(glmnet)
library(corrplot)
library(readr)
heart <- read_csv("heart.csv")
View(heart)
#missing value check
colSums(is.na(heart))
#data is clean there arent any missing values
#are our categorical data is also classified into numeric one, its important for us to separate them.
#lets divide the data into categorical,continous and target variable
cat_cols <- c('sex', 'exng', 'caa', 'cp', 'fbs', 'restecg', 'slp', 'thall')
con_cols <- c('age', 'trtbps', 'chol', 'thalachh', 'oldpeak')
target_col <- c('output')
# Print the column information
cat("The categorial variables are: ", cat_cols, "\n")
cat("The continuous variable are: ", con_cols, "\n")
cat("The target variable is: ", target_col, "\n")
# Splitting the dataset into training and testing sets
set.seed(42) # For reproducibility
library(caret)
splitIndex <- createDataPartition(heart$output, p = .8, list = FALSE, times = 1)
train_data <- heart[splitIndex,]
test_data <- heart[-splitIndex,]
# Standardize the features (excluding the target variable 'output')
preproc <- preProcess(train_data[,-ncol(train_data)], method = c("center", "scale"))
train_data_scaled <- predict(preproc, train_data[,-ncol(train_data)])
test_data_scaled <- predict(preproc, test_data[,-ncol(test_data)])
# Adding the output column back
train_data_scaled$output <- train_data$output
test_data_scaled$output <- test_data$output
# Fit the logistic regression model
glm_model <- glm(output ~ ., data = train_data_scaled, family = "binomial")
# Predict on the test set
predictions <- predict(glm_model, test_data_scaled, type = "response")
predicted_class <- ifelse(predictions > 0.5, 1, 0)
summary(glm_model)
# Evaluate the model
library(caret)
confusionMatrix(factor(predicted_class), factor(test_data_scaled$output))
plot(glm_model)
lasso_model <- glmnet(x_train, y_train, family = "binomial", alpha = 1)
ridge_model <- glmnet(x_train, y_train, family = "binomial", alpha = 0)
#
# Cross-validation for lasso
cv_lasso <- cv.glmnet(x_train, y_train, family = "binomial", alpha = 1)
cv_lasso
# Best lambda for lasso
best_lambda_lasso <- cv_lasso$lambda.min
# Predict with lasso
predictions_lasso <- predict(cv_lasso, s = best_lambda_lasso, newx = x_test, type = "response")
#lasso and ridge
library(glmnet)
x_train <- as.matrix(train_data_scaled[,-ncol(train_data_scaled)]) # Features matrix for training
y_train <- train_data_scaled$output # Response variable for training, ensure it's a factor for classification
x_test <- as.matrix(test_data_scaled[,-ncol(test_data_scaled)]) # Features matrix for testing
y_test <- test_data_scaled$output # Response variable for testing
lasso_model <- glmnet(x_train, y_train, family = "binomial", alpha = 1)
ridge_model <- glmnet(x_train, y_train, family = "binomial", alpha = 0)
#
# Cross-validation for lasso
cv_lasso <- cv.glmnet(x_train, y_train, family = "binomial", alpha = 1)
cv_lasso
# Best lambda for lasso
best_lambda_lasso <- cv_lasso$lambda.min
# Predict with lasso
predictions_lasso <- predict(cv_lasso, s = best_lambda_lasso, newx = x_test, type = "response")
# Convert probabilities to binary class (e.g., using 0.5 threshold)
predicted_class_lasso <- ifelse(predictions_lasso > 0.5, 1, 0)
# Evaluate lasso model
confusionMatrix(factor(predicted_class_lasso), factor(y_test))
# Cross-validation for ridge regression
cv_ridge <- cv.glmnet(x_train, y_train, family = "binomial", alpha = 0)
cv_ridge
# Best lambda for ridge regression
best_lambda_ridge <- cv_ridge$lambda.min
# Predict with ridge regression model using the best lambda
predictions_ridge <- predict(cv_ridge, s = best_lambda_ridge, newx = x_test, type = "response")
# Convert probabilities to binary class (e.g., using 0.5 threshold)
predicted_class_ridge <- ifelse(predictions_ridge > 0.5, 1, 0)
# Evaluate ridge regression model
confusionMatrix(factor(predicted_class_ridge), factor(y_test))
