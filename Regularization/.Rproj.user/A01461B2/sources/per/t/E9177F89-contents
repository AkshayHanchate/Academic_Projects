library(knitr)
library(ggplot2)
library(dplyr)
library(tidyverse)
library(RColorBrewer)
library(magrittr)
library(readxl)
library(kableExtra)
library(ggrepel)
library(tidyr)
library(leaps)
library(ISLR)
library(caret)
library(corrplot)
library(ggplot2)
library(glmnet)
library(corrplot)


library(readr)
heart <- read_csv("heart.csv")
View(heart)


#missing value check 
colSums(is.na(heart))
#data is clean there arent any missing values 

#are our categorical data is also classified into numeric one, its important for us to separate them. 

#lets divide the data into categorical,continous and target variable 
cat_cols <- c('sex', 'exng', 'caa', 'cp', 'fbs', 'restecg', 'slp', 'thall')
con_cols <- c('age', 'trtbps', 'chol', 'thalachh', 'oldpeak')
target_col <- c('output')

# Print the column information
cat("The categorial variables are: ", cat_cols, "\n")
cat("The continuous variable are: ", con_cols, "\n")
cat("The target variable is: ", target_col, "\n")


# Question 1: Strongest correlation with heart attacks
correlation_matrix <- cor(heart)
corrplot(correlation_matrix, method = "color")

#Question 2: Determine whether it is a classification or regression model
# As we are predicting heart diesease, classification modeling will be used. 

#Checking for outliers for Age
library(reshape2)
# Create box plots for numerical variables to identify outliers
numericals <- c("age", "trtbps", "chol", "thalachh", "oldpeak")

# Create box plots
boxplot_data <- melt(heart[, c("output", numericals)], id.vars = "output")
ggplot(boxplot_data, aes(x = variable, y = value, color = factor(output))) +
  geom_boxplot() +
  facet_wrap(~variable, scales = "free_y", ncol = 1) +
  labs(title = "Boxplot of Numerical Variables by Heart Disease Status") +
  theme_minimal()


#gettting the summary of the data 
summary(heart)

# Splitting the dataset into training and testing sets
set.seed(42) # For reproducibility
library(caret)
splitIndex <- createDataPartition(heart$output, p = .8, list = FALSE, times = 1)
train_data <- heart[splitIndex,]
test_data <- heart[-splitIndex,]

# Standardize the features (excluding the target variable 'output')
preproc <- preProcess(train_data[,-ncol(train_data)], method = c("center", "scale"))
train_data_scaled <- predict(preproc, train_data[,-ncol(train_data)])
test_data_scaled <- predict(preproc, test_data[,-ncol(test_data)])

# Adding the output column back
train_data_scaled$output <- train_data$output
test_data_scaled$output <- test_data$output

# Fit the logistic regression model
glm_model <- glm(output ~ ., data = train_data_scaled, family = "binomial")

# Predict on the test set
predictions <- predict(glm_model, test_data_scaled, type = "response")
predicted_class <- ifelse(predictions > 0.5, 1, 0)

# Evaluate the model
library(caret)
confusionMatrix(factor(predicted_class), factor(test_data_scaled$output))
plot(glm_model)


#lasso and ridge

library(glmnet)
x_train <- as.matrix(train_data_scaled[,-ncol(train_data_scaled)]) # Features matrix for training
y_train <- train_data_scaled$output # Response variable for training, ensure it's a factor for classification

x_test <- as.matrix(test_data_scaled[,-ncol(test_data_scaled)]) # Features matrix for testing
y_test <- test_data_scaled$output # Response variable for testing

lasso_model <- glmnet(x_train, y_train, family = "binomial", alpha = 1)

ridge_model <- glmnet(x_train, y_train, family = "binomial", alpha = 0)

#
# Cross-validation for lasso
cv_lasso <- cv.glmnet(x_train, y_train, family = "binomial", alpha = 1)
cv_lasso
# Best lambda for lasso
best_lambda_lasso <- cv_lasso$lambda.min

# Predict with lasso
predictions_lasso <- predict(cv_lasso, s = best_lambda_lasso, newx = x_test, type = "response")

# Convert probabilities to binary class (e.g., using 0.5 threshold)
predicted_class_lasso <- ifelse(predictions_lasso > 0.5, 1, 0)

# Evaluate lasso model
confusionMatrix(factor(predicted_class_lasso), factor(y_test))

# Cross-validation for ridge regression
cv_ridge <- cv.glmnet(x_train, y_train, family = "binomial", alpha = 0)
cv_ridge
# Best lambda for ridge regression
best_lambda_ridge <- cv_ridge$lambda.min

# Predict with ridge regression model using the best lambda
predictions_ridge <- predict(cv_ridge, s = best_lambda_ridge, newx = x_test, type = "response")

# Convert probabilities to binary class (e.g., using 0.5 threshold)
predicted_class_ridge <- ifelse(predictions_ridge > 0.5, 1, 0)

# Evaluate ridge regression model
confusionMatrix(factor(predicted_class_ridge), factor(y_test))






#Now that our data is clean we can start with data modeling. 
#As we are trying to predict about heart attacks, we should be using classification models.

#Feature Engineering 
# Set the seed for reproducibility
set.seed(42)

# Assuming your data frame is named 'df' and the target variable is named 'output'
# Create indices for train and test sets
indices <- createDataPartition(heart$output, p = 0.8, list = FALSE)

# Split the data into training and testing sets
train_data <- heart[indices, ]
test_data <- heart[-indices, ]

# Separate predictors (X) and target variable (y) for training set
train_x <- train_data[, !(names(train_data) %in% "output")]
train_y <- train_data$output

# Separate predictors (X) and target variable (y) for testing set
test_x <- test_data[, !(names(test_data) %in% "output")]
test_y <- test_data$output

# Print the shapes of the datasets
cat("The shape of train_x is ", dim(train_x), "\n")
cat("The shape of train_y is ", length(train_y), "\n")
cat("The shape of test_x is ", dim(test_x), "\n")
cat("The shape of test_y is ", length(test_y), "\n")


# Install and load the required library
library(rpart)

# Assuming your data frame is named 'df' and the target variable is named 'output'
# Assuming train_x, train_y, test_x, and test_y are already defined

# Combine predictors and target variable for training set
train_data <- cbind(train_x, output = train_y)

# Fit the decision tree model
dt_model <- rpart(output ~ ., data = train_data, method = "class")

# Print the decision tree
printcp(dt_model)

# Plot the decision tree (you may need to install the 'rpart.plot' package)
library(rpart.plot)
rpart.plot(dt_model, main = "Decision Tree")

# Make predictions on the test set
predictions_tree <- predict(dt_model, newdata = test_x, type = "class")

# Evaluate the performance of the model
conf_matrix_tree <- table(predictions_tree, test_y)
print(conf_matrix_tree)



# Feature Selection (Choose the features you want to include)
selected_features <- c("age", "trtbps", "chol", "thalachh", "oldpeak")

# Subset the data with selected features
train_data <- train_data[, selected_features]
test_data <- test_data[, selected_features]

# Standardize/Scale the features (optional but recommended for k-means)
train_data_scaled <- scale(train_data)
test_data_scaled <- scale(test_data)

# Perform k-means clustering
num_clusters <- 2  # You can adjust the number of clusters
kmeans_result <- kmeans(train_data_scaled, centers = num_clusters)

# Access the cluster assignments
cluster_assignments <- kmeans_result$cluster

# Add cluster assignments to the training data
train_data_clustered <- cbind(train_data, Cluster = cluster_assignments)

# Display the clustered data
print(train_data_clustered)

plot(train_data_scaled, col = kmeans_result$cluster)


#As we are not collecting time stamped data, we cannot perform time series analysis 


# Question 3: Age contribution towards heart attack

heart <- heart %>%
  mutate(Output_Label = ifelse(output == 1, "Yes", "No"))

ggplot(heart, aes(x = age)) + geom_histogram(binwidth = 5, fill = "blue", color = "black", alpha = 0.7)



# Question 4: Cholesterol level as a risk factor

ggplot(heart, aes(x = chol, y = thalachh)) +
  geom_point() +
  labs(title = "Scatter Plot of Cholesterol and Max Heart Rate",
       x = "Cholesterol",
       y = "Max Heart Rate")